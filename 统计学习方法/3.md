# 统计学习方法　李航
## 第三章 k近邻法
- **k近邻法**：一种基本的分类与回归方法（本书只讲了分类）。
    - 分类原理：利用训练数据集对特征向量空间进行划分，作为其分类模型
    - 三要素
        - k值的选择:    可以使用交叉验证的方法确定k值
        - 距离度量：     比如欧式距离，lp距离等  p39
        - 分类决策规则：   往往是多数表决，原理是经验风险最小化  p40
    - 算法描述:　p37(其实就是训练集中找出离实例最近的k个样本，这k个样本中最多样本点所属的种类就是结果)

- **kd树**（一种存储结构，可以提高k近邻搜索的效率）      p41
    - 构造平衡kd树（例子见课本，平衡不一定指子节点体积相同，一般指空间中包含的实例数量相同）
        - 根节点为K维全部空间；
        - 依次选定一个维度坐标轴的中点，将父节点切分为两半，每一半为一个子节点（子区域），实例这个维度坐标小于切分点保留在左子节点区域，大于保存在右子节点区域，等于保存在父节点里；
        - 一直切分下去，直到子区域中没有实例时终止，终止时节点为叶节点；


- 基于kd树的**搜索**（包含目标点的叶节点对应包含目标点的最小超矩形区域）    p43
    - 找到对应叶节点：从根节点出发，递归向下访问，直到子节点为叶节点为止；
    - 以此叶节点中的实例作为“当前最近点”；
    - 递归地向上回退，在每个节点进行以下操作；
        - 如果父节点保存的实例点比当前最近点更近，则以该实例点为“当前最近点”
        - 检查最近邻球体是否与兄弟节点相交，如果相交检查另一子节点中是否有更近的实例点
    - 当回退到根节点时，搜索结束。
    
