# 统计学习方法　李航
## 第六章 逻辑斯谛回归与最大熵模型
> 逻辑斯谛回归模型与最大熵模型都是属于对数线性模型
### 逻辑斯谛回归模型
- 逻辑斯谛分布 (p77)
    - 这是指对于一个变量的一种概率分布形式，分布函数是relu函数，密度函数是其导数

- 二项逻辑斯谛回归模型 (p78)
    - 这是一种二分类概率模型，指当固定x后，y等于0或1具有一个关于x的固定概率形式
    - 这种概率模型输出y=1的对数几率是输入x的线性函数表示的模型，这也叫逻辑斯谛回归模型
    - 在这种模型中，x的线性函数的值越接近正无穷，y=1可能性越大，越接近负无穷，y=0的可能性越大

- 模型参数估计  (p79)
    - 首先是建立损失函数
    - 然后基于损失函数最小化的思想求解最优化问题
    - 基本方法是梯度下降发和拟牛顿法

- 多项逻辑斯谛回归 (p79)
    - 这是一种多分类模型
    - 与二项逻辑斯谛回归思想类似，具体可看书

### 最大熵模型
- 最大熵原理：学习概率模型时，在所有可能的概率模型分布中，熵最大的模型是最好的模型
    - 也可以表述为在满足约束条件的模型集合中选取熵最大的模型
    - 熵机计算公式  (p80)
    - 在满足约束条件后，最大熵原理认为，所有不确定的部分都是等可能的，此时熵也是最大的

- 最大熵模型  (p83)
    - 首先列出所有满足约束条件的模型集合
    - 然后找出上述集合中条件熵最大的模型称为最大熵模型

- 最大熵模型的学习  (p83)
    - 首先将最大熵问题转换成最小值问题
    - 然后通过求解最优化问题进行求解

- 极大似然估计  (87)
    - 对偶函数的极大化等价于最大熵模型的极大似然估计证明

### 模型学习的最优化算法
> 我们已经遇到了好多求解目标函数最后化的问题，这些问题的求解方式通常一致，都是使用迭代算法进行求解。一般来说这些目标函数都是光滑的凸函数。很多最优化方法都有效

- 改进的迭代尺度法(improved iterative scaling, IIS)   (p90)
    - 不求梯度，更新的值是通过解方程解出来的

- 拟牛顿法
