## pytorch提供的自动梯度功能
在[上一篇](1.md)的最后我们使用基本的tensor类型实现了一个简单的三层神经网络，可以发现其中关于梯度的后向传播是比较繁琐的，这种简单的神经网络还好，如果换成现在复杂的深度神经网络，手动的梯度后向传播会麻烦到让人脑袋爆炸。

pytorch作为如今主流的深度学习框架之一，其提供了非常方便的自动梯度功能。本文主要介绍`torch.autogard`包，使用这个包，网络参数会自动进行优化过程中需要用到的梯度值的计算，极大的方便了开发。

> 代码请见同目录下[test2](test.py)

### `torch.autograd.Variable`可以自动计算梯度的节点类
- 如果想要使用自动梯度传播，我们需要将一个网络模型中的所有计算节点都封装成`torch.autograd.Variable`，如图，封装前后的节点其实还是Tensor（多维矩阵）类型
```python
>>> x = torch.randn(2, 2)
>>> x
tensor([[ 0.1757, -0.0816],
        [-0.1460, -0.7028]])
>>> x1 = Variable(x, requires_grad = False)
>>> x1
tensor([[ 0.1757, -0.0816],
        [-0.1460, -0.7028]])
>>> x2 = Variable(x, requires_grad = True)
>>> x2
tensor([[ 0.1757, -0.0816],
        [-0.1460, -0.7028]], requires_grad=True)
>>> x3 = x2-x1
>>> x3
tensor([[0., 0.],
        [0., 0.]], grad_fn=<SubBackward0>)
```
---

如果使用自动梯度优化一个三层神经网络，具体流程如下：
- 将网络节点都封装成`torch.autograd.Variable`，需要自动计算梯度的节点设置成`requires_grad = True`
```python
#x为输入，y为输出节点
x = Variable(torch.randn(batch_n, input_data), requires_grad = False)
y = Variable(torch.randn(batch_n, output_data), requires_grad = False)

#w1与w2为连接三层网络的两个参数节点，这也是需要进行自动梯度调整的地方
w1 = Variable(torch.randn(input_data, hidden_layer), requires_grad = True)
w2 = Variable(torch.randn(hidden_layer, output_data), requires_grad = True)
```
---

- 一个完整迭代过程如下，定义整个前向计算过程，并计算出loss，针对loss进行自动后向传播，并依据计算好的梯度进行参数更新
```python
#直接将整个前向运算整个写出来    
y_pred = x.mm(w1).clamp(min=0).mm(w2)
loss = (y_pred-y).pow(2).sum()

#自动进行后向传播
loss.backward()

#后向传播后，每个参数节点中会存放梯度数据
w1.data -= learning_rate*w1.grad.data
w2.data -= learning_rate*w2.grad.data

#更新后，将各个参数节点的梯度值全部置零
w1.grad.data.zero_()
w2.grad.data.zero_()
```
---

-  
