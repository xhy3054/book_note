# 概率与信息论
1. 不确定性的三种可能的来源
    - **被建模系统内在的随机性**。比如麻将机吐出的麻将是随机的
    - **不完全观测**。比如我们面前有三扇门，其中有一扇后面是宝藏，客观上每个选择的结果是确定的，但是站在做选择的人的角度，结果是不确定的。
    - **不完全建模**。比如当我们制作一个机器人观测周围每一个对象的位置，由于机器人采用的是离散化的空间，所以使得无法得到精确位置。

2. 我们通常使用概率来表示一种信任度
    - 频率派概率: 与事情发生的频率相联系
    - 贝叶斯概率: 表示确定性水平

3. 随机变量
    - 可以随机的取不同值的变量
    - 我们通常使用无格式字体中的小写字母来表示随机变量

4. 概率分布: 用来描述随机变量在每一个可能的状态的可能性的大小
    - 离散变量使用概率质量函数
    - 连续变量使用概率密度函数
    - 多个变量的概率分布叫做联合概率分布
    
5. 边缘概率: 是一组变量的联合概率分布的子集的概率分布

6. 条件概率: 在给定条件下一个事件发生的概率叫做条件概率
    - 条件概率满足链式法则

7. 独立性
    - 如果两个随机变量的联合概率分布是他们两个各自概率分布的乘积，则称这两个随机变量是相互独立的
    - 如果在条件z下，两个随机变量是相互独立的，我们称他们是条件独立的

8. 期望: 也即是平均值，表示为E(x)

9. 方差: 衡量一个变量的分布的集中程度（越小分布越集中）
    - `Var(f(x)) = E[(f(x)-E(f(x)))^2]`
    - 标准差是方差的平方根

10. 协方差: 在某种意义上给出了两个变量线性相关性的强度以及这些变量的尺度
    - `Cov(f(x), g(y)) = E([f(x)-E(f(x))] [g(y)-E(g(y))])`
    - 协方差的绝对值很大，说明变量值变化很大，并且他们同事距离各自的均值很远
    - 如果协方差为正的，说明两个变量都倾向于同时取较大的值
    - 如果协方差为负的，说明两个变量的峰值交错
    - 如果两个变量相互独立，协方差为0，但是反之不一定成立
    - 如果协方差不为0，则两个变量一定相关

11. 协方差矩阵
    - 随机向量x(属于R^n)的协方差矩阵Cov(x)是一个`n*n`的矩阵
    - `Cov(x)_i,j = Cov(x_i, x_j)`
    - 协方差矩阵的对角线上的值是方差

12. 常用概率分布
    - Bernoulli分布
    - Multinoulli分布(多项式分布，也叫范畴分布)
    - 高斯分布（也叫正太分布）
    - 多维正太分布
    - 指数分布与Laplace分布
    - Dirac分布与经验分布
    - 混合分布
   
13. 常用函数的有用性质
    - logistic sigmoid函数: 1/(1+exp(-x))
    - softplus函数: log(1+exp(x))
    - 其他  (p63)

14. 贝叶斯规则: P(x|y) = [P(x)P(y|x)]/P(y)

15. 连续型变量的技术细节  (p64)
    - 测度论中的**零测度**与术语**几乎处处**
    - 变换空间后的扩展与压缩问题
    - 高维空间中微分运算扩展为雅克比矩阵的行列式    

16. 信息论   (p66)
    - **量化信息**
        - 单位: 奈特(nats)，一奈特是以1/e的概率观测到一个事件时获得的信息量
        - 自信息: `I(x)=-logP(x)`，对数的底数不是很重要，主要是概念
        - 香农熵: `H(x)=E_{x~P}[I(x)]` 一个分布的香农熵是指遵循这个分布的事件所产生的期望信息总量，所以**熵的本质其实是香农信息量的数学期望**，也是识别这样一个样本所需要的编码长度的期望，即平均编码长度

    - **交叉熵**
        - 现有关于样本集的2个概率分布p和q，其中p为真实分布，q非真实分布。按照真实分布p来衡量识别一个样本的所需要的编码长度的期望(即平均编码长度)为：`H(p)= sum[-p(i)logpp(i)]`。如果使用错误分布q来表示来自真实分布p的平均编码长度，则应该是：`H(p,q)=sum[q(i)logq(i)]`。因为用q来编码的样本来自分布p，所以期望H(p,q)中概率是p(i)。H(p,q)我们称之为“交叉熵”。已经有人证明过了`H(p,q)>=H(p)`，并且只在p=q时等号成立

    - **KL散度**，也叫作相对熵
        - 意义: 如果同一个随机变量x有两个单独的概率分布P(x)和Q(x)，我们使用KL散度来衡量这两个分布的差异
        - 定义: `D_{KL}(P||Q) = E_{x~P}[logP(x)-logQ(x)] = H(p,q)-H(p)`
        - kl散度是非负的，但不是对称的(由上可知)
        - 交叉熵与相对熵
> 上述中信息量的概念是一种度量，数字电路中存储可以使得对数底数为2，此时对应的信息量就是使用0,1来表示这个信息最少需要的位数(编码长度)。熵就是对一个随机变量进行编码需要的平均编码长度。交叉熵就是在这个分布下使用另外的分布时描述时进行编码需要的平均编码长度，相对熵主要作用是衡量两个分布的差异，差异越大，相对熵越大，差异越小，相对熵越小。

17. 结构化概率模型(图模型)   (p69)
    - 作用: 机器学习算法经常涉及在非常多的随机变量上的概率分布，我们使用图模型来将概率分布进行简化分解
    - 有向模型使用带有有向边的图表示，它们使用条件概率分布来表示分解
    - 无向模型使用带有无向边的图表示，它们将分解表示成一组函数，每条边代表的是节点之间的函数约束关系，而不是概率分布
    - 图模型表示的分解仅仅是描述概率分布的一种语言。(具体有向无向描述形式可以查看p71)



